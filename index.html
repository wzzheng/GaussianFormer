<!doctype html>
<html>
<head>
<title>GaussianFormer</title>
<meta name="viewport" content="width=device-width,initial-scale=1">
<!-- <link href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous"> -->
<link href="bootstrap.min.css" rel="stylesheet">
<!-- <script src="https://code.jquery.com/jquery-3.2.1.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script> -->
<!-- <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet"> -->
<link href="opensans.css" rel="stylesheet">
<link rel="icon" href="images/logo3.png">
<link href="style.css" rel="stylesheet">
<style>
  .container_2{
    position: relative;
    width: 100%;
    height: 0;
    padding-bottom: 56.25%;
}
.video {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

  .collapsible {
    background-color: #777;
    color: white;
    cursor: pointer;
    padding: 18px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
  }

  .active, .collapsible:hover {
    background-color: #555;
  }
  
  .content {
    padding: 0 18px;
    max-height: 0;
    overflow: hidden;
    transition: max-height 0.2s ease-out;
    background-color: #f1f1f1;
  }
</style>

<style>
.paperthumb {
  float:left; width: 120px; margin: 3px 10px 7px 0;
}
.paperdesc {
  clear: both;
}
</style>
</head>

<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <p class="lead" style="font-size:30px">
    <b><a href="https://arxiv.org/abs/">GaussianFormer: Scene as Gaussians for Vision-Based <br> 3D Semantic Occupancy Prediction</a></b>
  <address style="font-size: 110%;">
    <nobr><a href="https://scholar.google.com/citations?&user=LKVgsk4AAAAJ">Yuanhui Huang</a>,</nobr>
    <nobr><a href="https://wzzheng.net/">Wenzhao Zheng</a><sup>†</sup>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?user=UgadGL8AAAAJ&hl=zh-CN&oi=ao">Yunpeng Zhang</a>,</nobr>
    <nobr><a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en">Jie Zhou</a>,</nobr>
    <nobr><a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>‡</sup>
  <br>
      <nobr>Tsinghua University</nobr>, 
      <nobr>UC Berkeley</nobr>
      <!-- <nobr><sup>2</sup>PhiGent Robotics</nobr> -->
  </address>
   <!-- <div style="font-size: 170%;">CVPR 2023</div> -->
  <address style="font-size: 120%;">
	 <!-- <br> -->
  [<a href="https://arxiv.org/pdf/2302.07817"><b>Paper (Arxiv)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://www.youtube.com/">Video(Youtube)</a>]&nbsp;&nbsp;&nbsp;&nbsp; -->
  [<a href="https://github.com/huang-yh/GaussianFormer"><b>Code (GitHub)</b></a>]&nbsp;&nbsp;&nbsp;&nbsp;
  <!-- [<a href="https://zhuanlan.zhihu.com">Post(Zhihu)</a>] -->
  </address>
  <small>† Project Leader. ‡Corresponding author.</small>
 </div>
 </p>
 </div>
</div> <!-- end nd-pageheader -->

<div class="container">


<p align="center">
  <video width="90%" controls>
    <source src="videos/demo.mp4" type="video/mp4">
  </video>
</p>

<p align="center">
    <img src="images/teaser.png" width="90%">
</p>
<p><b>Overview of our contributions.</b> 
  Considering the universal approximating ability of Gaussian mixture, we propose an object-centric 3D semantic Gaussian representation to describe the fine-grained structure of 3D scenes without the use of dense grids. 
  We propose a GaussianFormer model consisting of sparse convolution and cross-attention to efficiently transform 2D images into 3D Gaussian representations. 
  To generate dense 3D occupancy, we design a Gaussian-to-voxel splatting module that can be efficiently implemented with CUDA. 
  With comparable performance, our GaussianFormer reduces memory consumption of existing 3D occupancy prediction methods by 75.2% - 82.2%.
</p>


<!-- <h2>Abstract</h2><hr>
<p>Modern methods for vision-centric autonomous driving perception widely adopt the bird's-eye-view (BEV) representation to describe a 3D scene.
  Despite its better efficiency than voxel representation, it has difficulty describing the fine-grained 3D structure of a scene with a single plane.
  To address this, we propose a tri-perspective view (TPV) representation which accompanies BEV with two additional perpendicular planes.
  We model each point in the 3D space by summing its projected features on the three planes. 
  To lift image features to the 3D TPV space, we further propose a transformer-based TPV encoder (TPVFormer) to obtain the TPV features effectively.
  We employ the attention mechanism to aggregate the image features corresponding to each query in each TPV plane. 
  Experiments show that our model trained with sparse supervision effectively predicts the semantic occupancy for all voxels.
  We demonstrate for the first time that using only camera inputs can achieve comparable performance with LiDAR-based methods on the LiDAR segmentation task on nuScenes.</p> -->


<h2>Object-centric 3D Scene Representation (3D Semantic Gaussians)</h2><hr>
<p>
  The voxel representation assigns each voxel in the 3D space with a feature and is redundant due to the sparsity nature of the 3D space.
  BEV and TPV employ 2D planes to describe 3D space but can only alleviate the redundancy issue.
  Differently, the proposed object-centric 3D Gaussian representation can adapt to flexible regions of interest yet can still describe the fine-grained structure of the 3D scene due to the strong approximating ability of mixing Gaussians.
</p>

<p align="center">
    <img src="images/comparisons.png" width="90%">
</p>
<p>
  Each Gaussian represents a flexible region of interest and consists of the mean, covariance, and its semantic category.
</p>



<h2>GaussianFormer: Image to Gaussians</h2><hr>
<p> 
  We randomly initialize a set of queries to instantiate the 3D Gaussians and adopt the cross-attention mechanism to aggregate information from multi-scale image features.
We iteratively refine the properties of the 3D Gaussians for smoother optimizations.
To efficiently incorporate interactions among 3D Gaussians, we treat them as point clouds located at the Gaussian means and leverage 3D sparse convolutions to process them. 
We then decode the properties of 3D semantic Gaussians from the updated queries as the scene representation.
  <p>

<p align="center">
     <img src="images/overview.png" width="90%">
</p>

Motivated by the 3D Gaussian splatting method in image rendering, we design an efficient Gaussian-to-voxel splatting module that aggregates neighboring Gaussians to generate the semantic occupancy for a certain 3D position.

<p align="center">
  <img src="images/splatting.png" width="60%">
</p>


<h2>Results</h2><hr>

<p>The proposed 3D Gaussian representation uses a sparse and adaptive set of features to describe a 3D  scene but can still model the fine-grained structure due to the universal approximating ability of Gaussian mixtures </p>

<h4>nuScenes</h4><hr>

3D semantic occupancy prediction results on nuScenes validation set. 
While the original TPVFormer is trained with LiDAR segmentation labels, TPVFormer* is supervised by dense occupancy annotations. 
Our method achieves comparable performance with state-of-the-art methods.


<p></p>

<p align="center">
  <img src="images/nuscenes.png" width="90%">
</p>
<!-- <b>Visualization results on 3D semantic occupancy prediction and nuScenes LiDAR segmentation.</b> Our method can generate more comprehensive prediction results than the LiDAR segmentation ground truth. -->


<h4>SSCBench-KITTI-360</h4><hr>

Our method achieves performance on par with state-of-the-art methods, excelling at some smaller and general categories (i.e. motorcycle, other-veh.).


<p></p>

<p align="center">
  <img src="images/sscbench.png" width="90%">
</p>


<h4>Efficiency Comparisons</h4><hr>

The latency and memory consumption for GaussianFormer are tested on one NVIDIA 4090 GPU with batch size one, while the results for other methods are reported in OctreeOcc tested on one NVIDIA A100 GPU.
Our method demonstrates significantly reduced memory usage compared to other representations.


<p></p>

<p align="center">
  <img src="images/efficiency.png" width="90%">
</p>


<h4>Visualizations</h4><hr>

We visualize the 3D Gaussians by treating them as ellipsoids centered at the Gaussian means with semi-axes determined by the Gaussian covariance matrices.
Our GussianFormer not only achieves reasonable allocation of resources, but also captures the fine details of object shapes.


<p></p>

<p align="center">
  <img src="images/vis.png" width="90%">
</p>



<p>
<div class="card">
<h3 class="card-header">Bibtex</h3>
<div class="card-block">
<pre class="card-text clickselect">
  @article{huang2024gaussian,
    title={GaussianFormer: Scene as Gaussians for Vision-Based 3D Semantic Occupancy Prediction},
    author={Huang, Yuanhui and Zheng, Wenzhao and Zhang, Yunpeng and Zhou, Jie and Lu, Jiwen},
    journal={arXiv preprint arXiv:},
    year={2023}
}
</pre>
</div>
</div>
</p>


<p align="right">
     <a href="https://hanlab.mit.edu/projects/anycost-gan/">Website Template</a>
</p>

</div>
</div> <!-- row -->

</div> <!-- container -->

<script>
  var coll = document.getElementsByClassName("collapsible");
  var i;
  
  for (i = 0; i < coll.length; i++) {
    coll[i].addEventListener("click", function() {
      this.classList.toggle("active");
      var content = this.nextElementSibling;
      if (content.style.maxHeight){
        content.style.maxHeight = null;
      } else {
        content.style.maxHeight = content.scrollHeight * 50+ "px";
      } 
      content.style.height = "550%";
    });
  }
</script>

</body>
</html>


